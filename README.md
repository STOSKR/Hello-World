# ğŸ® CS-Tracker

Web scraper automÃ¡tico para rastrear precios y oportunidades de arbitraje de skins de CS2 desde [SteamDT](https://steamdt.com/hanging).

## ğŸš€ CaracterÃ­sticas

- âœ… **Scraping AutomÃ¡tico**: Extrae datos cada 6 horas usando GitHub Actions
- âœ… **Base de Datos en la Nube**: Almacena historial en Supabase (PostgreSQL)
- âœ… **Sin Costos**: Stack 100% gratuito
- âœ… **Multi-usuario**: Acceso compartido a datos
- âœ… **Historial Completo**: Rastrea cambios de precios a lo largo del tiempo

## ğŸ› ï¸ Stack TecnolÃ³gico

- **Python 3.11** - Lenguaje principal
- **Playwright** - Web scraping con navegador real
- **Supabase** - Base de datos PostgreSQL en la nube (gratis)
- **GitHub Actions** - AutomatizaciÃ³n y scheduling (gratis)

## ğŸ“‹ Requisitos Previos

1. **Cuenta de Supabase** (gratis)
   - RegÃ­strate en [supabase.com](https://supabase.com)
   - Crea un nuevo proyecto
   
2. **Python 3.11+**
3. **Git**

## âš™ï¸ InstalaciÃ³n y ConfiguraciÃ³n

### 1. Clonar el Repositorio

```bash
git clone https://github.com/STOSKR/Cs-Tracker.git
cd Cs-Tracker
```

### 2. Configurar Entorno Virtual (Recomendado)

```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux/Mac
python -m venv venv
source venv/bin/activate
```

### 3. Instalar Dependencias

```bash
pip install -r requirements.txt
playwright install chromium
```

### 4. Configurar Supabase

#### A. Crear la Base de Datos

1. Ve a tu proyecto en [Supabase Dashboard](https://app.supabase.com)
2. Navega a **SQL Editor**
3. Copia y pega el contenido de `config/schema.sql`
4. Ejecuta la query

#### B. Obtener Credenciales

1. En tu proyecto Supabase, ve a **Settings â†’ API**
2. Copia:
   - **Project URL** (ejemplo: `https://xyzproject.supabase.co`)
   - **anon public key** (el token largo que empieza con `eyJ...`)

#### C. Configurar Variables de Entorno

```bash
# Copiar el template
cp .env.example .env

# Editar .env y aÃ±adir tus credenciales
# SUPABASE_URL=tu_url_aqui
# SUPABASE_KEY=tu_key_aqui
```

### 5. Configurar GitHub Actions (AutomatizaciÃ³n)

Para que el scraper se ejecute automÃ¡ticamente cada 6 horas:

1. Ve a tu repositorio en GitHub
2. **Settings â†’ Secrets and variables â†’ Actions**
3. AÃ±ade estos **Repository Secrets**:
   - `SUPABASE_URL`: Tu URL de Supabase
   - `SUPABASE_KEY`: Tu anon key de Supabase

**Â¡Listo!** El scraper se ejecutarÃ¡ automÃ¡ticamente:
- â° Cada 6 horas (00:00, 06:00, 12:00, 18:00 UTC)
- ğŸ”§ TambiÃ©n puedes ejecutarlo manualmente desde la pestaÃ±a "Actions"

## ğŸ§ª Prueba Local

Antes de dejar que GitHub Actions lo ejecute automÃ¡ticamente, pruÃ©balo localmente:

```bash
# AsegÃºrate de tener el .env configurado
python src/main.py
```

Si todo funciona, deberÃ­as ver:
- Logs del navegador abriendo steamdt.com
- Datos extraÃ­dos
- ConfirmaciÃ³n de guardado en Supabase

## ğŸ“ Estructura del Proyecto

```
Cs-Tracker/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ scraper.yml          # ConfiguraciÃ³n de GitHub Actions
â”œâ”€â”€ config/
â”‚   â””â”€â”€ schema.sql               # Schema de la base de datos
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scraper.py               # LÃ³gica de web scraping
â”‚   â”œâ”€â”€ database.py              # ConexiÃ³n con Supabase
â”‚   â””â”€â”€ main.py                  # Script principal
â”œâ”€â”€ .env.example                 # Template de variables de entorno
â”œâ”€â”€ .gitignore                   # Archivos ignorados por git
â”œâ”€â”€ requirements.txt             # Dependencias de Python
â””â”€â”€ README.md                    # Este archivo
```

## ğŸ” Uso de la Base de Datos

### Consultas BÃ¡sicas (SQL)

Puedes ejecutar estas queries en el **SQL Editor** de Supabase:

```sql
-- Ver Ãºltimos 10 items scrapeados
SELECT * FROM scraped_items 
ORDER BY scraped_at DESC 
LIMIT 10;

-- Ver historial de un item especÃ­fico
SELECT item_name, buy_price, sell_price, scraped_at
FROM scraped_items 
WHERE item_name LIKE '%AK-47%'
ORDER BY scraped_at DESC;

-- Ver Ãºltimos precios Ãºnicos de cada item
SELECT * FROM latest_items;

-- Detectar cambios de precio en las Ãºltimas 24h
SELECT * FROM get_price_changes(24);
```

### Desde Python

```python
from src.database import SupabaseDB

db = SupabaseDB()

# Obtener Ãºltimos 100 items
items = db.get_latest_items(limit=100)

# Historial de un item especÃ­fico
history = db.get_item_history("AK-47 | Redline", limit=50)

# Items en un rango de fechas
items = db.get_items_by_date_range(
    start_date="2025-10-01T00:00:00",
    end_date="2025-10-31T23:59:59"
)
```

## ğŸ“Š Monitoreo

### GitHub Actions

1. Ve a la pestaÃ±a **Actions** en tu repositorio
2. VerÃ¡s el historial de ejecuciones
3. Click en cualquier ejecuciÃ³n para ver logs detallados

### Logs

Si ejecutas localmente, los logs se guardan en:
- `scraper.log` - Archivo de log
- `data/latest_scrape.json` - Ãšltimo scraping en JSON

## ğŸ› Troubleshooting

### Error: "supabase module not found"
```bash
pip install supabase
```

### Error: "playwright not installed"
```bash
playwright install chromium
```

### Error: "SUPABASE_URL not set"
- Verifica que el archivo `.env` existe
- Verifica que las variables estÃ¡n correctamente configuradas
- Para GitHub Actions, verifica los Secrets

### El scraper no encuentra datos
- La estructura del sitio web puede haber cambiado
- Abre un issue con los logs
- Revisa `data/latest_scrape.json` para ver quÃ© se extrajo

## ğŸ”„ PersonalizaciÃ³n

### Cambiar Frecuencia de Scraping

Edita `.github/workflows/scraper.yml`:

```yaml
schedule:
  # Cada 3 horas
  - cron: '0 */3 * * *'
  
  # Cada dÃ­a a las 9 AM UTC
  - cron: '0 9 * * *'
  
  # Cada hora
  - cron: '0 * * * *'
```

### Ajustar Selectores CSS

Si el sitio cambia su estructura, edita `src/scraper.py` en el mÃ©todo `_extract_items()`.

## ğŸ“ Notas Importantes

- âš ï¸ **Respeta los tÃ©rminos de servicio** del sitio que scrapeeas
- ğŸ”’ **Nunca commitees** el archivo `.env` con tus credenciales
- ğŸ’¾ **Supabase gratuito** tiene lÃ­mite de 500MB
- â±ï¸ **GitHub Actions** tiene 2000 minutos/mes gratis (mÃ¡s que suficiente)

## ğŸ¤ Contribuciones

Â¡Las contribuciones son bienvenidas!

1. Fork el proyecto
2. Crea una rama (`git checkout -b feature/nueva-funcionalidad`)
3. Commit tus cambios (`git commit -m 'AÃ±adir nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Abre un Pull Request

## ğŸ“„ Licencia

Este proyecto es de cÃ³digo abierto y estÃ¡ disponible bajo la licencia MIT.

## ğŸ”— Links Ãštiles

- [Supabase Docs](https://supabase.com/docs)
- [Playwright Docs](https://playwright.dev/python/)
- [GitHub Actions Docs](https://docs.github.com/actions)
- [SteamDT](https://steamdt.com/hanging)

---

Desarrollado con â¤ï¸ para la comunidad de CS2

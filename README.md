# ğŸ® CS-Tracker - Sistema Inteligente de Arbitraje CS2

**Trabajo Fin de MÃ¡ster**: Sistema de arbitraje financiero automatizado para skins de CS2 utilizando IA AgÃ©ntica.

## ğŸ¯ VisiÃ³n del Proyecto

Pipeline inteligente de decisiÃ³n autÃ³noma que detecta, analiza y ejecuta oportunidades de arbitraje entre Steam y Buff163 usando **LangGraph** (orquestaciÃ³n) y **Pydantic-AI** (inteligencia artificial).

### Objetivos del Sistema

1. **ğŸ” DetecciÃ³n**: Identificar diferencias de precio (arbitraje) entre mercados en tiempo real
2. **ğŸ“Š Filtrado**: Seleccionar oportunidades matemÃ¡ticamente rentables (ROI > X%)
3. **ğŸ¤– ValidaciÃ³n IA**: Analizar riesgo y tendencias usando LLMs (Gemini/GPT)
4. **âš¡ EjecuciÃ³n**: Realizar operaciones de forma autÃ³noma (simulada o real)

## ğŸ—ï¸ Arquitectura

**Clean Architecture** con separaciÃ³n estricta de responsabilidades:

```
app/
â”œâ”€â”€ core/           # ConfiguraciÃ³n transversal (Settings, Logger)
â”œâ”€â”€ domain/         # LÃ³gica pura (Models, State, Rules)
â”œâ”€â”€ services/       # LÃ³gica de negocio (Scraping, Math, Storage)
â”œâ”€â”€ graph/          # OrquestaciÃ³n LangGraph
â”‚   â”œâ”€â”€ nodes/      # Nodos especializados (Scout, Math, Analyst)
â”‚   â”œâ”€â”€ agents/     # Agentes Pydantic-AI
â”‚   â””â”€â”€ workflow.py # DefiniciÃ³n del grafo
â””â”€â”€ main.py         # Entrypoint
```

## ğŸš€ CaracterÃ­sticas Principales

### Fase 1: Sistema Base (Implementado)
- âœ… **Scraping AutomÃ¡tico**: Extrae datos cada 6 horas usando GitHub Actions
- âœ… **Base de Datos en la Nube**: Almacena historial en Supabase (PostgreSQL)
- âœ… **Sin Costos**: Stack 100% gratuito
- âœ… **Multi-usuario**: Acceso compartido a datos
- âœ… **Historial Completo**: Rastrea cambios de precios a lo largo del tiempo

### Fase 2-4: Sistema AgÃ©ntico (Roadmap)
- ğŸ”„ **OrquestaciÃ³n LangGraph**: Flujo de decisiÃ³n cÃ­clico y resiliente
- ğŸ”„ **Agentes IA**: ValidaciÃ³n de riesgo con LLMs estructurados
- ğŸ”„ **Persistencia MongoDB**: Almacenamiento de oportunidades validadas
- ğŸ”„ **EjecuciÃ³n AutÃ³noma**: Trading automÃ¡tico basado en anÃ¡lisis IA

## ğŸ› ï¸ Stack TecnolÃ³gico

### Core System (Implementado)
| Componente | TecnologÃ­a | PropÃ³sito |
|------------|-----------|-----------|
| Lenguaje | **Python 3.11+** | Async nativo, tipado estricto |
| Web Scraping | **Playwright** | Navegador real, JavaScript dinÃ¡mico |
| Base de Datos | **Supabase** | PostgreSQL en la nube (gratis) |
| AutomatizaciÃ³n | **GitHub Actions** | Scheduling y CI/CD (gratis) |

### AI System (Roadmap - TFM)
| Componente | TecnologÃ­a | PropÃ³sito |
|------------|-----------|-----------|
| OrquestaciÃ³n | **LangGraph** | GestiÃ³n de estado y flujo cÃ­clico |
| Agentes IA | **Pydantic-AI** | LLMs con output estructurado |
| Modelos LLM | **Gemini Flash / GPT-4o-mini** | Baja latencia, bajo costo |
| Cliente HTTP | **httpx** | Async, HTTP/2, proxies |
| Base de Datos | **MongoDB** | Persistencia asÃ­ncrona |
| ConfiguraciÃ³n | **pydantic-settings** | GestiÃ³n de .env |
| Testing | **pytest + pytest-asyncio** | Tests unitarios e integraciÃ³n |

## ğŸ“‹ Requisitos Previos

1. **Cuenta de Supabase** (gratis)
   - RegÃ­strate en [supabase.com](https://supabase.com)
   - Crea un nuevo proyecto
   
2. **Python 3.11+**
3. **Git**

## âš™ï¸ InstalaciÃ³n y ConfiguraciÃ³n

### 1. Clonar el Repositorio

```bash
git clone https://github.com/STOSKR/Cs-Tracker.git
cd Cs-Tracker
```

### 2. Configurar Entorno Virtual (Recomendado)

```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux/Mac
python -m venv venv
source venv/bin/activate
```

### 3. Instalar Dependencias

```bash
pip install -r requirements.txt
playwright install chromium
```

### 4. Configurar Supabase

#### A. Crear la Base de Datos

1. Ve a tu proyecto en [Supabase Dashboard](https://app.supabase.com)
2. Navega a **SQL Editor**
3. Copia y pega el contenido de `config/schema.sql`
4. Ejecuta la query

#### B. Obtener Credenciales

1. En tu proyecto Supabase, ve a **Settings â†’ API**
2. Copia:
   - **Project URL** (ejemplo: `https://xyzproject.supabase.co`)
   - **anon public key** (el token largo que empieza con `eyJ...`)

#### C. Configurar Variables de Entorno

```bash
# Copiar el template
cp .env.example .env

# Editar .env y aÃ±adir tus credenciales
# SUPABASE_URL=tu_url_aqui
# SUPABASE_KEY=tu_key_aqui
```

### 5. Configurar GitHub Actions (AutomatizaciÃ³n)

Para que el scraper se ejecute automÃ¡ticamente cada 6 horas:

1. Ve a tu repositorio en GitHub
2. **Settings â†’ Secrets and variables â†’ Actions**
3. AÃ±ade estos **Repository Secrets**:
   - `SUPABASE_URL`: Tu URL de Supabase
   - `SUPABASE_KEY`: Tu anon key de Supabase

**Â¡Listo!** El scraper se ejecutarÃ¡ automÃ¡ticamente:
- â° Cada 6 horas (00:00, 06:00, 12:00, 18:00 UTC)
- ğŸ”§ TambiÃ©n puedes ejecutarlo manualmente desde la pestaÃ±a "Actions"

## ğŸ§ª Prueba Local

Antes de dejar que GitHub Actions lo ejecute automÃ¡ticamente, pruÃ©balo localmente:

```bash
# AsegÃºrate de tener el .env configurado
python src/main.py
```

Si todo funciona, deberÃ­as ver:
- Logs del navegador abriendo steamdt.com
- Datos extraÃ­dos
- ConfirmaciÃ³n de guardado en Supabase

## ğŸ“ Estructura del Proyecto

```
Cs-Tracker/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ scraper.yml          # GitHub Actions (ejecuciÃ³n automÃ¡tica)
â”œâ”€â”€ config/
â”‚   â””â”€â”€ schema.sql               # Schema Supabase (PostgreSQL)
â”œâ”€â”€ src/                         # Sistema base (Fase 1)
â”‚   â”œâ”€â”€ scraper.py               # Web scraping con Playwright
â”‚   â”œâ”€â”€ database.py              # ConexiÃ³n Supabase
â”‚   â””â”€â”€ main.py                  # Script principal
â”œâ”€â”€ app/                         # Sistema agÃ©ntico (Fases 2-4)
â”‚   â”œâ”€â”€ core/                    # ConfiguraciÃ³n transversal
â”‚   â”‚   â”œâ”€â”€ config.py            # Settings (pydantic-settings)
â”‚   â”‚   â””â”€â”€ logger.py            # Logger JSON estructurado
â”‚   â”œâ”€â”€ domain/                  # LÃ³gica pura (sin I/O)
â”‚   â”‚   â”œâ”€â”€ models.py            # Pydantic Schemas (Skin, Offer, Analysis)
â”‚   â”‚   â”œâ”€â”€ state.py             # AgentState (LangGraph)
â”‚   â”‚   â””â”€â”€ rules.py             # FÃ³rmulas de fees y spread
â”‚   â”œâ”€â”€ services/                # LÃ³gica de negocio
â”‚   â”‚   â”œâ”€â”€ scraping.py          # Scrapers Steam/Buff (httpx)
â”‚   â”‚   â”œâ”€â”€ market_math.py       # CÃ¡lculos financieros
â”‚   â”‚   â””â”€â”€ storage.py           # Repositorio MongoDB
â”‚   â”œâ”€â”€ graph/                   # OrquestaciÃ³n LangGraph
â”‚   â”‚   â”œâ”€â”€ nodes/               # Scout, Math, Analyst, Trader
â”‚   â”‚   â”œâ”€â”€ agents/              # Agentes Pydantic-AI
â”‚   â”‚   â””â”€â”€ workflow.py          # DefiniciÃ³n del grafo
â”‚   â””â”€â”€ main.py                  # Entrypoint agÃ©ntico
â”œâ”€â”€ .env.example                 # Template variables de entorno
â”œâ”€â”€ requirements.txt             # Dependencias Python
â””â”€â”€ README.md                    # Este archivo
```

## ğŸ” Uso de la Base de Datos

### Consultas BÃ¡sicas (SQL)

Puedes ejecutar estas queries en el **SQL Editor** de Supabase:

```sql
-- Ver Ãºltimos 10 items scrapeados
SELECT * FROM scraped_items 
ORDER BY scraped_at DESC 
LIMIT 10;

-- Ver historial de un item especÃ­fico
SELECT item_name, buy_price, sell_price, scraped_at
FROM scraped_items 
WHERE item_name LIKE '%AK-47%'
ORDER BY scraped_at DESC;

-- Ver Ãºltimos precios Ãºnicos de cada item
SELECT * FROM latest_items;

-- Detectar cambios de precio en las Ãºltimas 24h
SELECT * FROM get_price_changes(24);
```

### Desde Python

```python
from src.database import SupabaseDB

db = SupabaseDB()

# Obtener Ãºltimos 100 items
items = db.get_latest_items(limit=100)

# Historial de un item especÃ­fico
history = db.get_item_history("AK-47 | Redline", limit=50)

# Items en un rango de fechas
items = db.get_items_by_date_range(
    start_date="2025-10-01T00:00:00",
    end_date="2025-10-31T23:59:59"
)
```

## ğŸ“Š Monitoreo

### GitHub Actions

1. Ve a la pestaÃ±a **Actions** en tu repositorio
2. VerÃ¡s el historial de ejecuciones
3. Click en cualquier ejecuciÃ³n para ver logs detallados

### Logs

Si ejecutas localmente, los logs se guardan en:
- `scraper.log` - Archivo de log
- `data/latest_scrape.json` - Ãšltimo scraping en JSON

## ğŸ› Troubleshooting

### Error: "supabase module not found"
```bash
pip install supabase
```

### Error: "playwright not installed"
```bash
playwright install chromium
```

### Error: "SUPABASE_URL not set"
- Verifica que el archivo `.env` existe
- Verifica que las variables estÃ¡n correctamente configuradas
- Para GitHub Actions, verifica los Secrets

### El scraper no encuentra datos
- La estructura del sitio web puede haber cambiado
- Abre un issue con los logs
- Revisa `data/latest_scrape.json` para ver quÃ© se extrajo

## ğŸ—ºï¸ Roadmap de ImplementaciÃ³n

El desarrollo sigue un enfoque incremental en 4 fases:

### ğŸŸ¢ Fase 1: Sistema Base (âœ… Completado)
- âœ… Scraping de SteamDT con Playwright
- âœ… Almacenamiento en Supabase
- âœ… AutomatizaciÃ³n con GitHub Actions
- âœ… Historial de precios
- **DoD**: Sistema funcional extrayendo y almacenando datos cada 6 horas

### ğŸŸ¡ Fase 2: Esqueleto del Grafo (En desarrollo)
- [ ] Definir `AgentState` en `domain/state.py`
- [ ] Crear nodo `scout_node` (extracciÃ³n de precios)
- [ ] Crear nodo `math_node` (filtrado por rentabilidad)
- [ ] Compilar grafo bÃ¡sico en `graph/workflow.py`
- **DoD**: Grafo funcional que calcula spreads y filtra oportunidades

### ğŸŸ¡ Fase 3: Inteligencia Artificial
- [ ] Configurar clientes Gemini/OpenAI
- [ ] Crear `analyst_agent` con Pydantic-AI
- [ ] Implementar validaciÃ³n de riesgo con LLM
- [ ] Integrar nodo `analyst_node` al grafo
- **DoD**: Sistema que genera anÃ¡lisis de riesgo estructurado por IA

### ğŸ”´ Fase 4: Persistencia y ProducciÃ³n
- [ ] Configurar MongoDB con Docker Compose
- [ ] Implementar `services/storage.py`
- [ ] Crear nodo `trader_node` (ejecuciÃ³n simulada)
- [ ] Logging y monitoreo completo
- **DoD**: Oportunidades validadas guardadas en BD, listas para ejecuciÃ³n

## ğŸ“ Principios de Desarrollo (TFM)

### Clean Code & Architecture
- **Tipado Estricto**: Type hints en todas las funciones
- **AsincronÃ­a**: Todo I/O es `async/await`
- **Nodos Ligeros**: LangGraph delega lÃ³gica a `services/`
- **InyecciÃ³n de Dependencias**: No instanciar clientes en funciones
- **ConfiguraciÃ³n Centralizada**: Todo en `core/config.py`

### Ejemplo de Nodo (LangGraph)
```python
async def scout_node(state: AgentState) -> AgentState:
    """Nodo responsable de buscar precios."""
    skin_name = state["target_skin"]
    try:
        market_data = await scraping_service.get_prices(skin_name)
        return {**state, "market_data": market_data}
    except Exception as e:
        return {**state, "errors": [f"Scraping error: {str(e)}"]}
```

### Ejemplo de Agente (Pydantic-AI)
```python
from pydantic_ai import Agent
from app.domain.models import RiskAnalysis

analyst = Agent(
    'google-gla:gemini-flash',
    result_type=RiskAnalysis,
    system_prompt="Analiza volatilidad y decide si es seguro comprar."
)

async def analyst_node(state: AgentState) -> AgentState:
    result = await analyst.run(f"Analiza: {state['spread_analysis']}")
    return {**state, "risk_assessment": result.data}
```

## ğŸ”„ PersonalizaciÃ³n

### Cambiar Frecuencia de Scraping

Edita `.github/workflows/scraper.yml`:

```yaml
schedule:
  # Cada 3 horas
  - cron: '0 */3 * * *'
  
  # Cada dÃ­a a las 9 AM UTC
  - cron: '0 9 * * *'
  
  # Cada hora
  - cron: '0 * * * *'
```

### Ajustar Selectores CSS

Si el sitio cambia su estructura, edita `src/scraper.py` en el mÃ©todo `_extract_items()`.

## ğŸ“Š Flujo del Sistema Completo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ENTRADA: Skin Target                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  ğŸ” Scout Node  â”‚  â† Extrae precios Steam/Buff
              â”‚  (Scraping)     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  ğŸ“Š Math Node   â”‚  â† Calcula spread, ROI, fees
              â”‚  (Filtering)    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ ROI > threshold? â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   YES â”‚    NO â†’ END
                       â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ ğŸ¤– Analyst Node â”‚  â† LLM analiza riesgo/tendencias
              â”‚  (Pydantic-AI)  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Riesgo BAJO?   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   YES â”‚    NO â†’ END
                       â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ âš¡ Trader Node  â”‚  â† Ejecuta operaciÃ³n
              â”‚   (Simulated)   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  ğŸ’¾ MongoDB     â”‚  â† Guarda oportunidad
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Notas Importantes

### LÃ­mites y Consideraciones
- âš ï¸ **Respeta los tÃ©rminos de servicio** del sitio que scrapeeas
- ğŸ”’ **Nunca commitees** el archivo `.env` con tus credenciales
- ğŸ’¾ **Supabase gratuito**: 500MB de espacio
- â±ï¸ **GitHub Actions**: 2000 minutos/mes gratis
- ğŸ¤– **Gemini Flash**: 15 RPM gratis, 1500 RPD
- ğŸ§  **GPT-4o-mini**: $0.15/1M tokens input

### Testing del Sistema
```powershell
# Fase 1: Test del scraper base
python src/main.py

# Fase 2: Test del grafo (cuando estÃ© implementado)
python -m pytest tests/test_graph.py

# Fase 3: Test del agente IA
python -m pytest tests/test_analyst_agent.py

# Fase 4: Test end-to-end
python app/main.py --skin "AK-47 | Redline"
```

## ğŸ¤ Contribuciones

Este proyecto es un **Trabajo Fin de MÃ¡ster** en desarrollo activo.

**Ãreas de contribuciÃ³n:**
- ğŸ› Reportar bugs en el scraper base
- ğŸ’¡ Sugerir mejoras en el sistema agÃ©ntico
- ğŸ§ª AÃ±adir tests y casos edge
- ğŸ“– Mejorar documentaciÃ³n

**Proceso:**
1. Fork el proyecto
2. Crea una rama (`git checkout -b feature/nueva-funcionalidad`)
3. Commit con mensajes descriptivos
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Abre un Pull Request

## ğŸ“„ Licencia

Este proyecto es de cÃ³digo abierto bajo licencia MIT.

**Disclaimer**: Este sistema es educativo. No se garantiza la rentabilidad ni se recomienda usar en producciÃ³n sin anÃ¡lisis de riesgo profesional.

## ğŸ”— Links Ãštiles

### DocumentaciÃ³n del Proyecto
- [Setup Guide](./SETUP.md) - GuÃ­a paso a paso de configuraciÃ³n
- [Schema SQL](./config/schema.sql) - Estructura de la base de datos
- [Examples](./examples/) - Scripts de ejemplo

### TecnologÃ­as Core
- [Supabase Docs](https://supabase.com/docs) - Base de datos PostgreSQL
- [Playwright Python](https://playwright.dev/python/) - Web scraping
- [GitHub Actions](https://docs.github.com/actions) - CI/CD

### TecnologÃ­as AI (TFM)
- [LangGraph](https://langchain-ai.github.io/langgraph/) - OrquestaciÃ³n de agentes
- [Pydantic-AI](https://ai.pydantic.dev/) - Agentes con output estructurado
- [Gemini API](https://ai.google.dev/gemini-api/docs) - LLM de Google
- [OpenAI API](https://platform.openai.com/docs) - GPT models

### Fuentes de Datos
- [SteamDT Hanging](https://steamdt.com/hanging) - Arbitraje Steam
- [Buff163](https://buff.163.com/) - Mercado secundario CS2

---

## ğŸ‘¨â€ğŸ’» Autor

**Trabajo Fin de MÃ¡ster** - Sistema de Arbitraje Inteligente con IA AgÃ©ntica

Desarrollado con â¤ï¸ para la comunidad de CS2 y entusiastas de IA

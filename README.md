# CS-Tracker

Sistema de arbitraje automatizado para skins de CS2 con IA. **Trabajo Fin de M√°ster**.

## Objetivos

Sistema inteligente que **detecta**, **analiza** y **ejecuta** oportunidades de arbitraje entre Steam y Buff163 usando **LangGraph** + **Pydantic-AI**.

1. Detectar diferencias de precio en tiempo real
2. Filtrar oportunidades rentables (ROI > X%)
3. Validar riesgo con LLMs (Gemini/GPT)
4. Ejecutar operaciones aut√≥nomamente

## Estado Actual

**Fase 1** (Implementado):
- Scraping autom√°tico cada 6 horas (GitHub Actions)
- Base de datos Supabase (PostgreSQL)
- Historial completo de precios
- 100% gratuito

**Fases 2-4** (Roadmap):
- Orquestaci√≥n con LangGraph
- Validaci√≥n IA con Pydantic-AI
- Trading aut√≥nomo

## Stack

**Implementado**: Python 3.11+ ‚Ä¢ Playwright ‚Ä¢ Supabase ‚Ä¢ GitHub Actions

**Roadmap**: LangGraph ‚Ä¢ Pydantic-AI ‚Ä¢ Gemini/GPT ‚Ä¢ MongoDB

## Quick Start

```bash
# 1. Clonar
git clone https://github.com/STOSKR/Cs-Tracker.git
cd Cs-Tracker

# 2. Instalar
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
playwright install chromium

# 3. Configurar Supabase
# - Crea cuenta en supabase.com
# - Ejecuta config/schema.sql en SQL Editor
# - Copia URL y API key

# 4. Variables de entorno
cp .env.example .env
# Edita .env con tus credenciales

# 5. Probar
python src/main.py
```

### GitHub Actions

A√±ade secrets en tu repo (Settings ‚Üí Secrets):
- `SUPABASE_URL`
- `SUPABASE_KEY`

El scraper se ejecutar√° autom√°ticamente cada 6 horas.

**Gu√≠a detallada**: [SETUP.md](./SETUP.md)

## Estructura

```
src/          # Fase 1 (scraping base - legacy funcional)
app/          # Fases 2-4 (arquitectura limpia)
  ‚îú‚îÄ‚îÄ core/       # Config, logging
  ‚îú‚îÄ‚îÄ domain/     # Models (SOLO ScrapedItem final), state, rules
  ‚îú‚îÄ‚îÄ services/   # Scraping (devuelve Dicts), c√°lculos, storage
  ‚îÇ   ‚îî‚îÄ‚îÄ extractors/  # ItemExtractor, DetailedItemExtractor ‚Üí Dict
  ‚îî‚îÄ‚îÄ graph/      # LangGraph nodes + agents (pendiente)
config/       # SQL schema, scraper_config.json
.github/      # CI/CD workflows
```

**Cambio clave**: `extractors/` devuelven `Dict`, solo `ScrapedItem` usa Pydantic.

## Uso

```python
from src.database import SupabaseDB

db = SupabaseDB()
items = db.get_latest_items(limit=100)
history = db.get_item_history("AK-47 | Redline")
```

```sql
-- SQL Editor en Supabase
SELECT * FROM latest_items;
SELECT * FROM get_price_changes(24);
```

## Roadmap

| Fase | Estado | Objetivo |
|------|--------|----------|
| **1. Base** | Completado | Scraping + Supabase + GitHub Actions |
| **2. Grafo** | En Progreso | LangGraph (Scout ‚Üí Math nodes) |
| **3. IA** | Pendiente | Pydantic-AI (Analyst agent) |
| **4. Producci√≥n** | Pendiente | MongoDB + Trading aut√≥nomo |

Detalles: [MASTER_PLAN.md](./MASTER_PLAN.md)

## Arquitectura (TFM)

**Clean Architecture** con separaci√≥n de responsabilidades:
- **Domain**: L√≥gica de negocio pura (sin dependencias externas)
- **Services**: Implementaciones concretas (I/O, APIs, DB)
- **Graph**: Orquestaci√≥n de flujos (LangGraph nodes)

### Filosof√≠a de Datos

**Simplicidad > Complejidad**: Usamos **Dicts simples** para datos intermedios y **Pydantic solo para validaci√≥n final**.

```python
# ‚úÖ Flujo de datos actual
# 1. ItemExtractor ‚Üí Dict (datos de tabla)
# 2. DetailedItemExtractor ‚Üí Dict (scraping detallado)
# 3. ScrapingService ‚Üí ScrapedItem (validaci√≥n Pydantic SOLO AL FINAL)

# ‚úÖ Ventajas:
# - Flexibilidad: f√°cil agregar/quitar campos sin tocar modelos
# - Performance: sin overhead de validaci√≥n en cada paso
# - Mantenibilidad: menos c√≥digo, menos bugs
# - Scraping-friendly: la estructura web cambia, los dicts se adaptan

# ‚ùå Evitado:
# - Pydantic en datos intermedios (Skin, MarketData, PriceData)
# - Validaci√≥n prematura que rompe el flujo
# - Rigidez de modelos en datos vol√°tiles
```

### Principios de C√≥digo Limpio

**1. Tipado Estricto**
```python
# ‚úÖ Correcto: Type hints expl√≠citos
async def get_prices(skin: str) -> MarketData:
    return MarketData(steam_price=10.5, buff_price=9.8)

# ‚ùå Incorrecto: Sin tipos
async def get_prices(skin):
    return {"steam": 10.5, "buff": 9.8}
```

**2. Asincron√≠a Nativa**
```python
# ‚úÖ Correcto: async/await para I/O
async def fetch_data(url: str) -> dict:
    async with httpx.AsyncClient() as client:
        response = await client.get(url)
        return response.json()

# ‚ùå Incorrecto: Operaciones bloqueantes
def fetch_data(url: str) -> dict:
    response = requests.get(url)  # Bloquea el event loop
    time.sleep(1)  # Nunca usar sleep en c√≥digo async
```

**3. Single Responsibility (Nodos Ligeros)**
```python
# ‚úÖ Correcto: Nodo delega l√≥gica
async def scout_node(state: AgentState) -> AgentState:
    """Nodo responsable solo de orquestar."""
    data = await scraping_service.get_prices(state["target_skin"])
    return {**state, "market_data": data}

# ‚ùå Incorrecto: L√≥gica compleja en el nodo
async def scout_node(state: AgentState) -> AgentState:
    async with httpx.AsyncClient() as client:
        # 50 l√≠neas de scraping...
        # Parsing HTML...
        # C√°lculos complejos...
```

**4. Inyecci√≥n de Dependencias**
```python
# ‚úÖ Correcto: Dependencias inyectadas
class ScrapingService:
    def __init__(self, http_client: httpx.AsyncClient, config: Settings):
        self.client = http_client
        self.config = config

# ‚ùå Incorrecto: Dependencias hardcodeadas
class ScrapingService:
    def __init__(self):
        self.client = httpx.AsyncClient()  # Dif√≠cil de testear
        self.api_key = os.getenv("KEY")  # Config dispersa
```

**5. Manejo de Errores Sin Crashes**
```python
# ‚úÖ Correcto: Errores en el estado
async def analyst_node(state: AgentState) -> AgentState:
    try:
        result = await ai_service.analyze(state["data"])
        return {**state, "analysis": result}
    except Exception as e:
        return {**state, "errors": [f"Analysis failed: {str(e)}"]}

# ‚ùå Incorrecto: Dejar que el grafo crashee
async def analyst_node(state: AgentState) -> AgentState:
    result = await ai_service.analyze(state["data"])  # Puede fallar
    return {**state, "analysis": result}
```

**6. Modelos Pydantic (Solo para validaci√≥n final)**
```python
# ‚úÖ Correcto: Dicts para datos intermedios, Pydantic al final
def extract_item(row) -> Dict:
    """Extrae datos de tabla (sin validaci√≥n)"""
    return {
        "item_name": row.text,
        "buff_url": row.link,
        "price": float(row.price)
    }

async def scrape_details(item: Dict) -> Dict:
    """Scraping detallado (sin validaci√≥n)"""
    return {
        **item,
        "steam_price": 10.5,
        "profit_eur": 2.3
    }

def finalize(data: Dict) -> ScrapedItem:
    """Validaci√≥n SOLO al final con Pydantic"""
    return ScrapedItem(**data)  # Aqu√≠ se valida todo

# ‚ùå Incorrecto: Pydantic en cada paso intermedio
class Skin(BaseModel):  # NO usar para datos intermedios
    name: str
    url: str

def extract_item(row) -> Skin:  # ‚ùå Validaci√≥n prematura
    return Skin(name=row.text, url=row.link)
```

**Raz√≥n**: En web scraping, la estructura cambia constantemente. Dicts son flexibles, Pydantic es r√≠gido. Validar solo al final asegura que los datos finales sean correctos sin romper el flujo.

**7. Configuraci√≥n Centralizada**
```python
# ‚úÖ Correcto: Settings con pydantic-settings
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    supabase_url: str
    gemini_api_key: str
    
    class Config:
        env_file = ".env"

settings = Settings()

# ‚ùå Incorrecto: os.getenv disperso
api_key = os.getenv("GEMINI_KEY")  # En cada archivo
```

**8. Sin Comentarios In√∫tiles ni Emojis**
```python
# ‚úÖ Correcto: C√≥digo autoexplicativo
async def extract_buff_prices(page: Page) -> List[Dict]:
    rows = await page.locator("tr.selling").all()
    return [await self._parse_row(row) for row in rows[:5]]

# ‚ùå Incorrecto: Comentarios redundantes
async def extract_buff_prices(page: Page) -> List[Dict]:
    """
    Extrae precios de BUFF
    
    Args:
        page: P√°gina de Playwright
    
    Returns:
        Lista de precios extra√≠dos
    """
    # Obtener todas las filas
    rows = await page.locator("tr.selling").all()
    # Retornar los primeros 5 elementos parseados
    return [await self._parse_row(row) for row in rows[:5]]

# ‚ùå Incorrecto: Emojis en logs de producci√≥n
logger.info("üöÄ Iniciando scraping...")
logger.error("‚ùå Error en BUFF")

# ‚úÖ Correcto: Logs limpios
logger.info("Iniciando scraping...")
logger.error("Error en BUFF")
```

**Regla**: Los comentarios deben explicar **por qu√©**, no **qu√©**. Si necesitas comentarios para explicar qu√© hace el c√≥digo, refactoriza. Los emojis a√±aden ruido visual y dificultan el parsing autom√°tico de logs.

### Ejemplos de Implementaci√≥n

```python
# Nodo LangGraph (Clean)
async def scout_node(state: AgentState) -> AgentState:
    """Extrae precios de mercados."""
    try:
        data = await scraping_service.get_prices(state["target_skin"])
        return {**state, "market_data": data}
    except Exception as e:
        return {**state, "errors": [f"Scraping: {str(e)}"]}

# Agente Pydantic-AI (Clean)
analyst = Agent(
    'gemini-flash',
    result_type=RiskAnalysis,
    system_prompt="Eres un analista financiero experto..."
)

async def analyst_node(state: AgentState) -> AgentState:
    """Valida riesgo con IA."""
    result = await analyst.run(f"Analiza: {state['spread']}")
    return {**state, "risk_assessment": result.data}
```

## Licencia

MIT License. Proyecto educativo - TFM.

**Disclaimer**: No garantiza rentabilidad. Solo fines educativos.

## Links

**Docs**: [SETUP.md](./SETUP.md) ‚Ä¢ [MASTER_PLAN.md](./MASTER_PLAN.md) ‚Ä¢ [Schema SQL](./config/schema.sql)

**Tech**: [Supabase](https://supabase.com/docs) ‚Ä¢ [Playwright](https://playwright.dev/python/) ‚Ä¢ [LangGraph](https://langchain-ai.github.io/langgraph/) ‚Ä¢ [Pydantic-AI](https://ai.pydantic.dev/)

---

**Trabajo Fin de M√°ster** - Sistema de Arbitraje con IA Ag√©ntica | MIT License

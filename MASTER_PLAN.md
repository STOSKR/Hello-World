# üéì MASTER PLAN - CS2 Agentic Graph System

**Trabajo Fin de M√°ster**: Sistema de Arbitraje Financiero Automatizado con IA Ag√©ntica

---

## üìå Estado del Proyecto

**‚úÖ Fase 1 COMPLETADA** - Sistema de scraping funcional en producci√≥n:
- Scraping autom√°tico con Playwright (headless/visible)
- Base de datos Supabase (PostgreSQL) con timestamps timezone-aware
- GitHub Actions (cada hora en :30 UTC) - repo p√∫blico con minutos ilimitados
- Sistema anti-ban configurable (2 workers concurrentes por defecto)
- Sesiones persistentes para BUFF/Steam (cookies guardadas como GitHub Secrets)
- ROI corregido: `((steam_price * 0.87) / buff_price) - 1`
- Async storage con worker dedicado para guardar items incrementalmente
- Producer-consumer pattern con Queue para procesamiento concurrente

**‚úÖ Fase 2 COMPLETADA** - Clean Architecture implementada:
- Migraci√≥n completa de `src/` a `app/` con separaci√≥n de capas
- Tipado estricto con Pydantic (solo para resultado final)
- Configuraci√≥n centralizada (JSON como source of truth)
- Logging estructurado con structlog
- C√≥digo deduplicado (~400 l√≠neas eliminadas)
- Performance optimizado (delays 1-2.5s vs 5-10s antes)

**‚è≥ Fases 3-4 PENDIENTES** - IA Ag√©ntica:
- LangGraph para orquestaci√≥n
- Pydantic-AI para validaci√≥n con LLMs
- Trading aut√≥nomo

---

## üéØ OBJETIVO

**ROL DE LA IA**: Principal Software Engineer y Arquitecto de IA

**MISI√ìN**: Refactorizar el sistema existente (`src/`) a Clean Architecture (`app/`) y extenderlo con:
- **LangGraph**: Orquestaci√≥n y gesti√≥n de estado
- **Pydantic-AI**: Inteligencia artificial con output estructurado

**EST√ÅNDAR**: C√≥digo de producci√≥n, Clean Architecture, As√≠ncrono y Tipado Estricto

---

## 1. üåü Visi√≥n y Objetivos

Desarrollar un **pipeline inteligente** donde el dato fluye a trav√©s de nodos especializados.

### Objetivos Principales

| # | Objetivo | Estado | Descripci√≥n |
|---|----------|--------|-------------|
| 1 | **Detecci√≥n en Tiempo Real** | ‚úÖ Completado | Scraping de Steam/Buff163 con Playwright cada hora (GitHub Actions) |
| 2 | **Filtrado Matem√°tico** | ‚úÖ Completado | C√°lculo de ROI, fees, spread con filtros configurables + async storage |
| 3 | **Validaci√≥n IA** | ‚è≥ Pendiente | Validar riesgo usando LLMs (Gemini/GPT) analizando tendencias |
| 4 | **Ejecuci√≥n Aut√≥noma** | ‚è≥ Pendiente | Ejecutar operaciones de trading de forma aut√≥noma |

---

## 2. üõ†Ô∏è Stack Tecnol√≥gico

### ‚úÖ Implementado (Fase 1-2 - `app/`)

| Componente | Tecnolog√≠a | Uso Actual |
|------------|-----------|------------|
| **Lenguaje** | Python 3.11+ | Async/await nativo, type hints everywhere |
| **Scraping** | Playwright | Navegaci√≥n headless/visible, anti-detecci√≥n, session persistence |
| **Base de Datos** | Supabase (PostgreSQL) | Almacenamiento hist√≥rico con timestamps timezone-aware (TEXT) |
| **CI/CD** | GitHub Actions | Ejecuci√≥n autom√°tica cada hora (:30 UTC) |
| **Configuraci√≥n** | pydantic-settings + JSON | Single source of truth (scraper_config.json) |
| **Logging** | structlog | JSON logging sin emojis |
| **CLI** | Click | Comandos: scrape, test-config, history, health |
| **Modelos** | Pydantic | Validaci√≥n estricta solo para ScrapedItem final |
| **Concurrency** | asyncio.Queue | Producer-consumer con 2 workers + storage worker |

### ‚è≥ Por Implementar (Fases 3-4)

| Componente | Tecnolog√≠a | Prop√≥sito |
|------------|-----------|----------|
| **Orquestaci√≥n** | LangGraph | Gesti√≥n de estado y flujo c√≠clico |
| **Agentes IA** | Pydantic-AI | LLMs con output estructurado |
| **Modelos LLM** | Gemini Flash / GPT-4o-mini | Low latency & cost |
| **Cliente HTTP** | httpx (opcional) | Async HTTP/2 para APIs REST |

---

## 3. üèóÔ∏è Arquitectura de Software (Clean Architecture)

**Principio**: El c√≥digo debe estar desacoplado. Los Nodos del Grafo NO contienen l√≥gica de negocio compleja, solo orquestan llamadas a Servicios.

### Estructura Actual (Fase 1-2 - Clean Architecture Implementada)

```
app/                        # Clean Architecture (COMPLETADA)
‚îú‚îÄ‚îÄ core/                   # Configuraci√≥n transversal
‚îÇ   ‚îú‚îÄ‚îÄ config.py           # Settings con pydantic-settings (JSON como source of truth)
‚îÇ   ‚îî‚îÄ‚îÄ logger.py           # Logger JSON estructurado con structlog
‚îú‚îÄ‚îÄ domain/                 # L√≥gica Pura (sin I/O)
‚îÇ   ‚îú‚îÄ‚îÄ models.py           # ScrapedItem, FilterConfig, AntibanConfig
‚îÇ   ‚îî‚îÄ‚îÄ rules.py            # F√≥rmulas (ROI corregido, fees, conversi√≥n CNY)
‚îú‚îÄ‚îÄ services/               # Implementaciones concretas
‚îÇ   ‚îú‚îÄ‚îÄ scraping.py         # Producer-consumer con async storage worker
‚îÇ   ‚îú‚îÄ‚îÄ storage.py          # Repositorio Supabase async (run_in_executor)
‚îÇ   ‚îú‚îÄ‚îÄ extractors/         # Buff, Steam, Item, Detailed extractors
‚îÇ   ‚îú‚îÄ‚îÄ filters/            # FilterManager
‚îÇ   ‚îî‚îÄ‚îÄ utils/              # BrowserManager (con session persistence)
‚îú‚îÄ‚îÄ graph/                  # LangGraph (Fases 3-4 - PENDIENTE)
‚îÇ   ‚îú‚îÄ‚îÄ nodes/              # Scout, Math, Analyst, Trader
‚îÇ   ‚îú‚îÄ‚îÄ agents/             # Pydantic-AI agents
‚îÇ   ‚îî‚îÄ‚îÄ workflow.py         # Compilaci√≥n del grafo
‚îî‚îÄ‚îÄ main.py                 # CLI con Click (scrape, test-config, history, health)

config/
‚îú‚îÄ‚îÄ scraper_config.json     # Single source of truth (headless, workers, delays)
‚îú‚îÄ‚îÄ sessions/               # Sesiones BUFF/Steam (gitignored, GitHub Secrets en CI)
‚îÇ   ‚îú‚îÄ‚îÄ buff_session.json
‚îÇ   ‚îî‚îÄ‚îÄ steam_session.json
‚îî‚îÄ‚îÄ schema.sql              # Schema Supabase actualizado

scripts/
‚îî‚îÄ‚îÄ save_session.py         # Script para guardar cookies localmente

.github/workflows/
‚îî‚îÄ‚îÄ scraper.yml             # Workflow horario con session loading

### Flujo de Datos

```
ENTRADA ‚Üí Scout Node ‚Üí Math Node ‚Üí Analyst Node ‚Üí Trader Node ‚Üí SALIDA
           (Scraping)   (Filtering)   (AI Risk)      (Execution)
```

---

## 4. üìã Fases de Implementaci√≥n (Roadmap)

### ‚úÖ FASE 1: Scraping Base y Almacenamiento (COMPLETADA)

**Objetivo**: Sistema funcional de scraping con almacenamiento persistente.

#### ‚úÖ Logros Completados
- ‚úÖ Scraper con Playwright (headless/visible configurable)
- ‚úÖ Integraci√≥n Supabase para historial de precios
- ‚úÖ Sistema de 6 presets de trading configurables
- ‚úÖ GitHub Actions (ejecuci√≥n autom√°tica cada 6 horas)
- ‚úÖ Anti-ban: concurrencia configurable (1-3 items paralelos)
- ‚úÖ Anti-ban: delays aleatorios entre requests
- ‚úÖ Anti-ban: 4 modos (safe/balanced/fast/stealth)
- ‚úÖ Guardado de progreso parcial en interrupciones
- ‚úÖ C√°lculo de fees Steam/Buff, spread, ROI, rentabilidad
- ‚úÖ Extracci√≥n de datos detallados (precios, vol√∫menes, listings)
- ‚úÖ Manejo robusto de errores con logging

#### Artefactos Existentes
- `src/scraper.py`: Scraper principal (330 l√≠neas)
- `src/database.py`: Cliente Supabase
- `src/main.py`: CLI con presets
- `config/scraper_config.json`: Config anti-ban
- `config/preset_configs.json`: Presets trading + anti-ban
- `set_anti_ban_mode.py`: CLI para cambiar modos
- `.github/workflows/`: GitHub Actions configurado

---

### ‚úÖ FASE 2: Migraci√≥n a Clean Architecture (COMPLETADA Diciembre 2025)

**Objetivo**: Refactorizar c√≥digo a `app/` siguiendo principios SOLID y optimizar performance.

#### ‚úÖ Logros Completados
- ‚úÖ Arquitectura limpia con separaci√≥n domain/services/core
- ‚úÖ Configuraci√≥n centralizada (JSON como source of truth, CLI solo overrides)
- ‚úÖ Logging estructurado con structlog (JSON sin emojis)
- ‚úÖ ROI corregido: `((steam_price * 0.87) / buff_price) - 1`
- ‚úÖ Performance optimizada:
  - Delays: 5-10s ‚Üí 1-2.5s
  - Timeouts: BUFF 30s‚Üí15s, Steam 10s
  - Default workers: 1 ‚Üí 2 concurrentes
- ‚úÖ Producer-consumer pattern con asyncio.Queue
- ‚úÖ Async storage worker implementado (c√≥digo existe, no habilitado por defecto)
- ‚úÖ Code deduplication: ~400 l√≠neas eliminadas
  - Unified `scrape_items()` method con `async_storage` parameter
  - Helper `_format_item_display()` para eliminar repetici√≥n
- ‚úÖ GitHub Actions optimizado:
  - Schedule: cada hora en :30 UTC (`cron: '30 * * * *'`)
  - Repo p√∫blico ‚Üí minutos ilimitados
  - Artifacts subidos siempre (logs + data)
- ‚úÖ DB Schema fix: `scraped_at` cambiado a TEXT para soportar ISO timestamps
- ‚úÖ CLI mejorado con Click:
  - `scrape`: scraping principal
  - `test-config`: validar configuraci√≥n
  - `history`: ver historial de items
  - `health`: health check de Supabase
- ‚úÖ Browser con persistent profile local (cookies autom√°ticas en `.cs_tracker_profile/`)

#### ‚ö†Ô∏è Pendientes/No Implementados
- ‚è≥ Session persistence para GitHub Actions (storage_state en BrowserManager)
  - **Raz√≥n**: Persistent profile funciona localmente, pero CI necesita approach diferente
  - **Soluci√≥n futura**: Implementar `storage_state` parameter cuando sea necesario acceder a sell history en CI
- ‚è≥ Script `save_session.py` completamente funcional
  - **Raz√≥n**: Problemas de red con BUFF163 (ERR_NETWORK_CHANGED)
  - **Workaround**: Usar persistent profile local por ahora

#### Artefactos Creados
- `app/core/config.py`: Settings con pydantic-settings
- `app/core/logger.py`: structlog JSON logging
- `app/domain/models.py`: ScrapedItem, FilterConfig, AntibanConfig
- `app/domain/rules.py`: calculate_roi(), convert_cny_to_eur()
- `app/services/scraping.py`: Producer-consumer con async storage worker
- `app/services/storage.py`: Async Supabase con run_in_executor
- `app/services/utils/browser_manager.py`: Persistent profile (local sessions autom√°ticas)
- `scripts/save_session.py`: Script para guardar cookies (WIP - problemas de red con BUFF)
- `.github/workflows/scraper.yml`: Workflow horario (sin session loading por ahora)

#### Definition of Done
- [x] ItemExtractor devuelve `List[Dict]` en lugar de objetos Pydantic
- [x] DetailedItemExtractor trabaja con `Dict` en lugar de `Skin`
- [x] ScrapingService valida con Pydantic solo al final (ScrapedItem)
- [x] Eliminados modelos innecesarios (Skin, MarketData, PriceData)
- [x] Logging estructurado sin emojis implementado
- [x] ROI formula corregida con Steam fee 13%
- [x] Async storage worker para guardar items durante scraping (c√≥digo implementado, usar `--no-async-storage` para deshabilitar)
- [x] Code deduplication completado (~400 l√≠neas)
- [x] GitHub Actions schedule optimizado (horario)
- [x] DB schema actualizado (scraped_at ‚Üí TEXT)
- [x] Persistent profile para sesiones locales autom√°ticas

---

### ‚è≥ FASE 3: Orquestaci√≥n con LangGraph (PENDIENTE)

**Objetivo**: Implementar grafo de nodos para flujo de decisi√≥n.

#### Tareas
- [ ] Definir `AgentState` en `app/domain/state.py`
- [ ] Crear `app/graph/nodes/scout_node.py` (orquesta scraping)
  - Delega a `services/scraping.py`
  - < 15 l√≠neas, solo orquestaci√≥n
- [ ] Crear `app/graph/nodes/math_node.py` (filtra por rentabilidad)
  - Usa `domain/rules.py` para c√°lculos
- [ ] Crear `app/graph/workflow.py` (compila grafo)
  - Define aristas Scout ‚Üí Math
- [ ] Integrar con servicios existentes de Fase 2

#### Definition of Done
- Ejecutar grafo con un skin devuelve estado con precios y profit
- Manejo de errores sin crashes (errores en `state['errors']`)
- Logging estructurado de cada transici√≥n de nodo
- Tests de integraci√≥n del flujo completo

---

### ‚è≥ FASE 4: Inteligencia Artificial con Pydantic-AI (PENDIENTE)

**Objetivo**: Validaci√≥n de riesgo usando LLMs.

#### Tareas
- [ ] Configurar cliente Gemini Flash / GPT-4o-mini
- [ ] Crear `app/graph/agents/analyst_agent.py` con Pydantic-AI
- [ ] Definir System Prompt ("Trader experto en CS2...")
- [ ] Crear `analyst_node` que consume el agente
  - Input: `state['market_data']` y `state['spread_analysis']`
  - Output: `state['risk_assessment']`
- [ ] Integrar an√°lisis de volatilidad hist√≥rica
- [ ] Integrar an√°lisis de volumen de mercado
- [ ] A√±adir `trader_node` (simulado) que ejecuta si riesgo LOW

#### Definition of Done
Sistema devuelve an√°lisis estructurado:
```json
{
  "risk_level": "LOW|MEDIUM|HIGH",
  "confidence": 0.85,
  "reasoning": "Volatilidad baja (3%), volumen alto (200/d√≠a)...",
  "recommended_action": "BUY|WAIT|SKIP"
}
```
- Operaciones simuladas se guardan en Supabase
- Logs de todas las decisiones del LLM
- Rate limiting para evitar costos excesivos

---

## 5. üìê Gu√≠a de Estilos y Buenas Pr√°cticas

Sigue estas reglas **estrictamente** al generar c√≥digo.

### 5.1. Tipado y Datos

‚úÖ **Hacer**:
```python
from typing import Dict, List

# Para datos intermedios: Dict con type hints
async def extract_items(page) -> List[Dict]:
    return [{"name": "AK-47", "price": 10.5}]

# Para datos finales: Pydantic validation
from pydantic import BaseModel

class ScrapedItem(BaseModel):
    item_name: str
    profit_eur: float

def finalize(data: Dict) -> ScrapedItem:
    return ScrapedItem(**data)  # Validaci√≥n SOLO aqu√≠
```

‚ùå **NO Hacer**:
```python
def extract_items(page):  # Sin tipos
    return ["AK-47", 10.5]  # Sin estructura

class Skin(BaseModel):  # NO para datos intermedios
    name: str

def extract(row) -> Skin:  # Validaci√≥n prematura
    return Skin(name=row.text)  # Rompe con cambios web
```

**Reglas**:
- **Datos intermedios**: `Dict` con type hints (`Dict`, `List[Dict]`)
- **Datos finales**: `Pydantic` para validaci√≥n (ScrapedItem)
- **Return Types**: Todas las funciones deben tener type hinting expl√≠cito
- **Raz√≥n**: Web scraping requiere flexibilidad, validar solo al final

---

### 5.2. LangGraph Patterns

‚úÖ **Nodos Ligeros** (< 15 l√≠neas):
```python
async def scout_node(state: AgentState) -> AgentState:
    """Nodo responsable de buscar precios."""
    skin_name = state["target_skin"]
    try:
        market_data = await scraping_service.get_prices(skin_name)
        return {**state, "market_data": market_data}
    except Exception as e:
        return {**state, "errors": [f"Scraping error: {str(e)}"]}
```

**Reglas**:
- Un nodo NO debe tener m√°s de 15 l√≠neas
- Debe delegar la l√≥gica a `services/`
- **Manejo de Errores**: No lances excepciones, escribe en `state['errors']`

---

### 5.3. Asincron√≠a (Asyncio)

‚úÖ **Correcto**:
```python
async def fetch_prices(skin: str) -> MarketData:
    async with httpx.AsyncClient() as client:
        response = await client.get(f"/api/prices/{skin}")
        return MarketData(**response.json())
```

‚ùå **Incorrecto**:
```python
def fetch_prices(skin: str):  # Sin async
    response = requests.get(f"/api/prices/{skin}")  # Bloqueante
    time.sleep(1)  # ‚ùå Nunca usar time.sleep()
```

**Reglas**:
- Todo I/O (Red, Base de Datos, LLM) debe ser `async/await`
- Nunca uses `time.sleep()`, usa `await asyncio.sleep()`

---

### 5.4. Mantenibilidad

#### Inyecci√≥n de Dependencias

‚úÖ **Hacer**:
```python
async def scout_node(
    state: AgentState,
    scraper: ScrapingService  # Inyectado
) -> AgentState:
    data = await scraper.get_prices(state["target_skin"])
    return {**state, "market_data": data}
```

‚ùå **NO Hacer**:
```python
async def scout_node(state: AgentState) -> AgentState:
    scraper = ScrapingService()  # ‚ùå Instanciado dentro
    data = await scraper.get_prices(state["target_skin"])
```

#### Configuraci√≥n Centralizada

‚úÖ **Hacer**:
```python
from app.core.config import settings

api_key = settings.GEMINI_API_KEY
```

‚ùå **NO Hacer**:
```python
import os
api_key = os.getenv("GEMINI_API_KEY")  # ‚ùå En mitad del c√≥digo
```

---

## 6. üìö Ap√©ndice: Ejemplos de C√≥digo Esperado

### Ejemplo 1: Nodo de LangGraph

```python
from typing import Dict, Any
from app.domain.state import AgentState
from app.services.scraping import ScrapingService

async def scout_node(state: AgentState) -> AgentState:
    """
    Nodo responsable de extraer precios de mercados.
    
    Input: state['target_skin']
    Output: state['market_data'] or state['errors']
    """
    skin_name = state["target_skin"]
    
    try:
        # Delegamos al servicio (Clean Code)
        market_data = await ScrapingService.get_prices(skin_name)
        return {**state, "market_data": market_data}
    
    except Exception as e:
        # No crash, agregamos error al estado
        return {**state, "errors": [f"Scraping error: {str(e)}"]}
```

---

### Ejemplo 2: Agente Pydantic-AI

```python
from pydantic import BaseModel
from pydantic_ai import Agent
from app.domain.models import RiskAnalysis

class SpreadAnalysis(BaseModel):
    skin_name: str
    spread_percent: float
    volume_24h: int
    volatility: float

# Definir el agente con output estructurado
analyst = Agent(
    'google-gla:gemini-flash',
    result_type=RiskAnalysis,  # Fuerza respuesta JSON estructurada
    system_prompt=(
        "Eres un trader experto en CS2 skins. "
        "Analiza la volatilidad hist√≥rica y el volumen de mercado. "
        "Decide si es seguro ejecutar la operaci√≥n de arbitraje."
    )
)

async def analyst_node(state: AgentState) -> AgentState:
    """
    Nodo que valida el riesgo usando IA.
    
    Input: state['spread_analysis']
    Output: state['risk_assessment']
    """
    try:
        # El LLM recibe contexto estructurado
        analysis = state['spread_analysis']
        
        result = await analyst.run(
            f"Analiza esta oportunidad: "
            f"Skin: {analysis.skin_name}, "
            f"Spread: {analysis.spread_percent}%, "
            f"Volumen 24h: {analysis.volume_24h}, "
            f"Volatilidad: {analysis.volatility}"
        )
        
        return {**state, "risk_assessment": result.data}
    
    except Exception as e:
        return {**state, "errors": [f"AI analysis error: {str(e)}"]}
```

---

### Ejemplo 3: Servicio de Scraping

```python
import httpx
from typing import Optional
from app.domain.models import MarketData
from app.core.config import settings
from app.core.logger import logger

class ScrapingService:
    """Servicio as√≠ncrono para extraer precios de mercados."""
    
    @staticmethod
    async def get_prices(skin_name: str) -> MarketData:
        """
        Extrae precios de Steam y Buff163.
        
        Args:
            skin_name: Nombre del skin (ej: "AK-47 | Redline")
            
        Returns:
            MarketData con precios de ambos mercados
            
        Raises:
            httpx.HTTPError: Si la petici√≥n falla
        """
        async with httpx.AsyncClient(timeout=30.0) as client:
            # Steam
            steam_response = await client.get(
                f"{settings.STEAM_API_URL}/market/priceoverview",
                params={"market_hash_name": skin_name}
            )
            steam_response.raise_for_status()
            steam_data = steam_response.json()
            
            # Buff163
            buff_response = await client.get(
                f"{settings.BUFF_API_URL}/market/goods",
                params={"game": "csgo", "search": skin_name}
            )
            buff_response.raise_for_status()
            buff_data = buff_response.json()
            
            logger.info(f"Prices fetched for {skin_name}")
            
            return MarketData(
                skin_name=skin_name,
                steam_price=float(steam_data['lowest_price']),
                buff_price=float(buff_data['data']['items'][0]['sell_min_price']),
                timestamp=datetime.utcnow()
            )
```

---

### Ejemplo 4: Modelo de Dominio

```python
from pydantic import BaseModel, Field
from datetime import datetime
from typing import Optional, Literal

# NOTA: Solo modelos para resultado final y configuraci√≥n
# Datos intermedios usan Dict simple

class ScrapedItem(BaseModel):
    """Resultado final del scraping (√öNICA validaci√≥n Pydantic)."""
    item_name: str
    url: Optional[str] = None
    buff_url: Optional[str] = None
    steam_url: Optional[str] = None
    
    buff_avg_price_eur: float = Field(..., gt=0)
    steam_avg_price_eur: float = Field(..., gt=0)
    
    profit_eur: float
    profitability_ratio: float
    
    scraped_at: datetime = Field(default_factory=datetime.utcnow)

class FilterConfig(BaseModel):
    """Configuraci√≥n de filtros."""
    min_price: float = Field(default=20.0, ge=0)
    max_price: Optional[float] = None
    min_volume: int = Field(default=40, ge=0)
    platforms: dict[str, bool] = Field(default={"BUFF": True})

class RiskAnalysis(BaseModel):
    """Resultado del an√°lisis de riesgo por IA (Fase 4)."""
    risk_level: Literal["LOW", "MEDIUM", "HIGH"]
    confidence: float = Field(..., ge=0.0, le=1.0)
    reasoning: str
    recommended_action: Literal["BUY", "WAIT", "SKIP"]

class AgentState(BaseModel):
    """Estado compartido del grafo LangGraph (Fase 3)."""
    target_skin: str
    market_data: Optional[dict] = None  # Dict, no Pydantic
    spread_analysis: Optional[dict] = None
    risk_assessment: Optional[RiskAnalysis] = None
    errors: list[str] = Field(default_factory=list)
```

---

## 7. ‚úÖ Checklist de Calidad

Antes de considerar una fase completada, verificar:

### Code Quality
- [ ] Todos los archivos tienen docstrings
- [ ] Todas las funciones tienen type hints
- [ ] No hay `print()`, solo logging estructurado
- [ ] No hay `time.sleep()` en c√≥digo async
- [ ] Validaci√≥n Pydantic SOLO en datos finales

### Architecture
- [ ] Los nodos de LangGraph son < 15 l√≠neas
- [ ] La l√≥gica de negocio est√° en `services/`
- [ ] Los modelos est√°n en `domain/`
- [ ] La configuraci√≥n est√° centralizada en `core/`

### Testing
- [ ] Existe un test para cada fase
- [ ] Los tests son async (`pytest-asyncio`)
- [ ] Coverage > 70%

### Documentation
- [ ] README actualizado con la fase completada
- [ ] Ejemplos de uso en `examples/`
- [ ] Comentarios en c√≥digo complejo

---

## 8. üöÄ Comandos de Desarrollo

```bash
# Setup inicial
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt

# Desarrollo
python app/main.py --skin "AK-47 | Redline"

# Testing
pytest tests/ -v
pytest tests/ --cov=app --cov-report=html

# Linting
ruff check app/
mypy app/

# Docker (Fase 4)
docker-compose up -d mongodb
```

---

## 9. üìä M√©tricas de √âxito

| M√©trica | Objetivo | Fase |
|---------|----------|------|
| **Latencia Scraping** | < 2 segundos | Fase 1 |
| **Precisi√≥n C√°lculos** | 100% (tests) | Fase 1 |
| **Latencia Grafo** | < 5 segundos | Fase 2 |
| **Latencia LLM** | < 3 segundos | Fase 3 |
| **Disponibilidad** | > 99% | Fase 4 |
| **ROI Real** | > 5% (simulado) | Fase 4 |

---

**üéì Este documento es la gu√≠a maestra para el desarrollo del TFM.**

√öltima actualizaci√≥n: Diciembre 2025 (Fase 2 completada)

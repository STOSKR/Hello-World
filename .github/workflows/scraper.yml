name: CS Tracker Scraper

# Ejecutar automÃ¡ticamente cada hora
on:
  schedule:
    # Cron: minuto hora * * *
    # Ejecutar cada hora en punto (UTC)
    - cron: '30 * * * *'
  
  # TambiÃ©n permitir ejecuciÃ³n manual
  workflow_dispatch:

jobs:
  scrape-and-store:
    runs-on: ubuntu-latest
    
    steps:
    - name: ðŸ“¥ Checkout repository
      uses: actions/checkout@v4
    
    - name: ðŸ Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: ðŸ“¦ Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        playwright install chromium
        playwright install-deps
    
    - name: ðŸ” Setup environment variables
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
      run: |
        echo "SUPABASE_URL=$SUPABASE_URL" >> .env
        echo "SUPABASE_KEY=$SUPABASE_KEY" >> .env
    
    - name: ðŸš€ Run scraper
      run: |
        python -m app scrape --quiet
    
    - name: ðŸ“Š Upload logs and results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: scraper-logs-${{ github.run_number }}
        path: |
          logs/*.log
          data/*.json
        retention-days: 7
    
    - name: ðŸ“ˆ Summary
      if: always()
      run: |
        echo "## Scraping Job Summary" >> $GITHUB_STEP_SUMMARY
        echo "- **Status**: ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Time**: $(date)" >> $GITHUB_STEP_SUMMARY
        echo "- **Branch**: ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
